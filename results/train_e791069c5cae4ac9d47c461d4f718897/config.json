{"vocab_size": 16, "device": "cpu", "work_dir": "results", "batch_size": 256, "test_size": 4096, "task": {"max_variables": 12}, "model": {"emb_dim": 64, "bias": false, "mlp_bias": true, "ff_dim": 128, "num_layers": 2, "num_heads": [1, 1], "dropout": null, "mask": true, "mlp": [false, true], "layer_norm": false, "activation": [false, true], "pos_enc": "rotary", "pos_max_len": 256, "flash": true}, "training": {"optimizer": "adam", "lr": 0.001, "schedule": "triangle", "warmup_steps": 5000, "total_steps": 10000, "eval_iter": 100, "get_attn": 1000, "get_checkpoints": 500, "weight_decay": 0.01, "label_smoothing": 0.1, "freeze_value": false, "freeze_out": false, "identity_query": false}, "wandb": {"project": "Reasoning"}}