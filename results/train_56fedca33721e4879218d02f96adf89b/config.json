{"vocab_size": 16, "device": "cuda", "work_dir": "results", "batch_size": 64, "test_size": 4096, "task": {"max_variables": 12, "max_seq_len": 83}, "model": {"emb_dim": 64, "bias": false, "mlp_bias": true, "ff_dim": 128, "num_layers": 3, "num_heads": [1, 1, 1], "dropout": 0.1, "mlp": [true, true, true], "layer_norm": true, "activation": [true, true, true], "pos_enc": "rotary", "pos_max_len": 83, "flash": true}, "training": {"optimizer": "adamw", "lr": 0.0005, "schedule": "triangle", "warmup_steps": 20000, "pad_ignore": true, "total_steps": 60000, "eval_iter": 150, "get_attn": 1000, "get_checkpoints": 500, "weight_decay": 0.01, "label_smoothing": 0, "freeze_value": false, "freeze_out": false, "identity_query": false}, "wandb": {"project": "Reasoning"}}